{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering refers to the process of crafting well-structured and precise instructions or queries that are given to language models or AI systems to generate desired outputs. It involves formulating prompts in a way that guides the model's behavior and helps achieve specific tasks or generate accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering is like providing a clear roadmap to an AI language model. Just as you'd give detailed directions to someone to ensure they understand and complete a task correctly, prompt engineering involves crafting instructions that guide AI systems to generate the desired outcomes.\n",
    "\n",
    " Effective prompt engineering involves a balance between clarity and specificity, ensuring that the AI understands the context and requirements to provide relevant and useful responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with prompts, you interact with the LLM via an API or directly. You can configure a few parameters to get different results for your prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature, in the context of AI text generation, controls the randomness of output. A higher temperature value (e.g., 0.8) leads to more diverse and creative responses, while a lower value (e.g., 0.2) produces more focused and deterministic outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top_p (Top Probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top_p, also known as nucleus sampling, determines the set of most likely next words based on their cumulative probabilities. Setting a specific threshold (e.g., 0.8) allows the AI model to consider only the most probable options, enhancing output coherence while maintaining some level of diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prompt contains any of the following elements:\n",
    "\n",
    "* Instruction - a specific task or instruction you want the model to perform\n",
    "\n",
    "* Context - external information or additional context that can steer the model to better responses\n",
    "\n",
    "* Input Data - the input or question that we are interested to find a response for\n",
    "\n",
    "* Output Indicator - the type or format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Examples for Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Dependencies :\n",
    "\n",
    "* Install Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "! pip install openai\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple python function to call OpenAI Completion API and get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file into current environment \n",
    "load_dotenv()\n",
    "\n",
    "# If this file is opened in VS Code then add OPENAI_API_KEY in .env file else comment below lines\n",
    "# Get OpenAI API Key from Environment Variables\n",
    "OPENAI_API_KEY= os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# If this file is opened in Colab Notebook then add OpenAI API Key below else comment below line\n",
    "# Provide OpenAI API Key\n",
    "# OPENAI_API_KEY= \"<Your_OpenAI_API_Key>\"\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def get_completion(prompt):\n",
    "    \"\"\" GET completion from openai api\"\"\"\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "                engine=\"text-davinci-002\",  # Engine to use for completion\n",
    "                prompt=prompt,              # Prompt to generate completion\n",
    "                temperature=0,              # Higher temperature results in more random completions\n",
    "                n=1,                        # Number of completions to generate\n",
    "                max_tokens=1024             # 1024 is the maximum token limit \n",
    "            )\n",
    "    \n",
    "    # Fetch result\n",
    "    result = response.choices[0].text.strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's understand how we can use different prompts to leverage the capabilities of OpenAI's language model for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples for Text Summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the bodyâ€™s immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n",
    "\"\"\"\n",
    "\n",
    "# Define a prompt to summerize provided text\n",
    "prompt = f\"Summarize the following text in one sentence:\\n{input_text}\\n\\nSummary: \"\n",
    "\n",
    "# Get the response from completion API\n",
    "get_completion(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "input_text = \"\"\"\n",
    "Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.\n",
    "\"\"\"\n",
    "\n",
    "# Define a prompt to generate sentences containing product information\n",
    "prompt = f\"Extract the information from the following text:\\n{input_text}\\n\\n\"\n",
    "\n",
    "# Get the response from completion API\n",
    "get_completion(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"\"\"\n",
    "Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use..\n",
    "\"\"\"\n",
    "\n",
    "# Question to ask about the passage\n",
    "question = \"What was OKT3 originally sourced from?\"\n",
    "\n",
    "# Prompt to ask the question\n",
    "prompt=f\"Passage: {passage}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "# Print question and answer\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", get_completion(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "input_text = \"\"\"\n",
    "Classify the text into neutral, negative or positive. \n",
    "Text: I think the food was okay. \n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Get the response from completion API\n",
    "get_completion(input_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "input_text = \"\"\"\n",
    "/*\n",
    "Ask the user for their name and say \"Hello\"\n",
    "*/\n",
    "\"\"\"\n",
    "\n",
    "# Get the response from completion API\n",
    "get_completion(input_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot prompting is a technique in artificial intelligence where a model is able to perform tasks it hasn't been directly trained for. Instead of teaching the model about each task separately, you provide just description of the task or example that guides the model to understand the task and generate relevant outputs. You dont provide any example. This works because the model has learned a broad range of information during its training and can apply that knowledge to new tasks with appropriate cues.\n",
    "\n",
    "For instance, if an AI language model is asked to translate sentences from English to French, even though it hasn't been trained on this specific translation task, it can still perform reasonably well by using the patterns it learned during training on related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let us look into some basic example for Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example prompt and candidate labels\n",
    "prompt = '''Classify the text into neutral, negative or positive. \n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:'''\n",
    "\n",
    "# Get the response from completion API\n",
    "get_completion(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompting is an artificial intelligence technique that enables a model to perform tasks it hasn't been extensively trained on. Instead of needing a large amount of specific training data for each task, you can provide the model with a small number of examples or demonstrations. These examples serve as hints or templates, guiding the model on how to handle the task. The model leverages its existing knowledge to generalize from the few examples it's given and produce relevant outputs.\n",
    "\n",
    "For example, suppose you want an AI to summarize news articles on various topics. Instead of training it on countless articles, you can show the model a handful of example news summaries along with their corresponding articles. This limited exposure helps the AI understand the summarization task and apply its understanding to new articles it hasn't encountered before.\n",
    "\n",
    "Zero-Shot = No Example\n",
    "Few-Shot  = Multiple Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let us look into some basic example for Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example few-shot prompt and completions\n",
    "prompt = '''This is awesome! // Positive\n",
    "This is bad! // Negative\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //'''\n",
    "\n",
    "# Get the response from completion API\n",
    "get_completion(prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
